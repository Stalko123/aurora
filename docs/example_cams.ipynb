{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f57a10-13a1-4f66-a734-065fc16b17b2",
   "metadata": {},
   "source": [
    "# Predictions for Air Pollution\n",
    "\n",
    "In this example, we will download [CAMS air pollution data](https://ads.atmosphere.copernicus.eu/datasets/cams-global-atmospheric-composition-forecasts) for 11 June 2022 at 0.4 degrees resolution and run Aurora Air Pollution on this data.\n",
    "\n",
    "Running this notebook requires additional Python packages. You can install these as follows:\n",
    "\n",
    "```\n",
    "pip install cdsapi matplotlib\n",
    "```\n",
    "\n",
    "## Downloading the Data\n",
    "\n",
    "To begin with, register an account with the [Atmosphere Data Store](https://ads.atmosphere.copernicus.eu/) and create `$HOME/.cdsapirc` with the following content:\n",
    "\n",
    "```\n",
    "url: https://ads.atmosphere.copernicus.eu/api\n",
    "key: <API key>\n",
    "```\n",
    "\n",
    "You can find your API key on your account page.\n",
    "\n",
    "In order to be able to download CAMS data, you need to accept the terms of use on the [dataset page](https://ads.atmosphere.copernicus.eu/datasets/cams-global-atmospheric-composition-forecasts?tab=download).\n",
    "\n",
    "\n",
    "```{warning}\n",
    "The URL in `$HOME/.cdsapirc` is `https://ads.atmosphere.copernicus.eu/api`, which conflicts with the URL `https://cds.climate.copernicus.eu/api` needed to download e.g. ERA5 data. You will need to manually switch between the URLs.\n",
    "```\n",
    "\n",
    "We now download the CAMS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541bf19-024f-4c76-8666-9d559640156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cdsapi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads\")\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the static variables from HuggingFace.\n",
    "static_path = hf_hub_download(\n",
    "    repo_id=\"microsoft/aurora\",\n",
    "    filename=\"aurora-0.4-air-pollution-static.pickle\",\n",
    ")\n",
    "print(\"Static variables downloaded!\")\n",
    "\n",
    "# Download the surface-level variables.\n",
    "if not (download_path / \"2022-06-11-cams.nc.zip\").exists():\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(\n",
    "        \"cams-global-atmospheric-composition-forecasts\",\n",
    "        {\n",
    "            \"type\": \"forecast\",\n",
    "            \"leadtime_hour\": \"0\",\n",
    "            \"variable\": [\n",
    "                # Meteorological surface-level variables:\n",
    "                \"10m_u_component_of_wind\",\n",
    "                \"10m_v_component_of_wind\",\n",
    "                \"2m_temperature\",\n",
    "                \"mean_sea_level_pressure\",\n",
    "                # Pollution surface-level variables:\n",
    "                \"particulate_matter_1um\",\n",
    "                \"particulate_matter_2.5um\",\n",
    "                \"particulate_matter_10um\",\n",
    "                \"total_column_carbon_monoxide\",\n",
    "                \"total_column_nitrogen_monoxide\",\n",
    "                \"total_column_nitrogen_dioxide\",\n",
    "                \"total_column_ozone\",\n",
    "                \"total_column_sulphur_dioxide\",\n",
    "                # Meteorological atmospheric variables:\n",
    "                \"u_component_of_wind\",\n",
    "                \"v_component_of_wind\",\n",
    "                \"temperature\",\n",
    "                \"geopotential\",\n",
    "                \"specific_humidity\",\n",
    "                # Pollution atmospheric variables:\n",
    "                \"carbon_monoxide\",\n",
    "                \"nitrogen_dioxide\",\n",
    "                \"nitrogen_monoxide\",\n",
    "                \"ozone\",\n",
    "                \"sulphur_dioxide\",\n",
    "            ],\n",
    "            \"pressure_level\": [\n",
    "                \"50\",\n",
    "                \"100\",\n",
    "                \"150\",\n",
    "                \"200\",\n",
    "                \"250\",\n",
    "                \"300\",\n",
    "                \"400\",\n",
    "                \"500\",\n",
    "                \"600\",\n",
    "                \"700\",\n",
    "                \"850\",\n",
    "                \"925\",\n",
    "                \"1000\",\n",
    "            ],\n",
    "            \"date\": \"2022-06-11\",\n",
    "            \"time\": [\"00:00\", \"12:00\"],\n",
    "            \"format\": \"netcdf_zip\",\n",
    "        },\n",
    "        str(download_path / \"2022-06-11-cams.nc.zip\"),\n",
    "    )\n",
    "# Unpack the ZIP. It should contain the surface-level and atmospheric data in separate\n",
    "# files.\n",
    "if not (download_path / \"2022-06-11-cams-surface-level.nc\").exists():\n",
    "    with zipfile.ZipFile(download_path / \"2022-06-11-cams.nc.zip\", \"r\") as zf, open(\n",
    "        download_path / \"2022-06-11-cams-surface-level.nc\", \"wb\"\n",
    "    ) as f:\n",
    "        f.write(zf.read(\"data_sfc.nc\"))\n",
    "if not (download_path / \"2022-06-11-cams-atmospheric.nc\").exists():\n",
    "    with zipfile.ZipFile(download_path / \"2022-06-11-cams.nc.zip\", \"r\") as zf, open(\n",
    "        download_path / \"2022-06-11-cams-atmospheric.nc\", \"wb\"\n",
    "    ) as f:\n",
    "        f.write(zf.read(\"data_plev.nc\"))\n",
    "print(\"Surface-level and atmospheric variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d8aa4-a16b-481c-b7e5-66764d6e98f1",
   "metadata": {},
   "source": [
    "## Preparing a Batch\n",
    "\n",
    "We convert the downloaded data to an `aurora.Batch`, which is what the model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4fb22-475b-4156-94c8-e61c77a20539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata\n",
    "\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars = pickle.load(f)\n",
    "surf_vars_ds = xr.open_dataset(\n",
    "    download_path / \"2022-06-11-cams-surface-level.nc\", engine=\"netcdf4\", decode_timedelta=True\n",
    ")\n",
    "atmos_vars_ds = xr.open_dataset(\n",
    "    download_path / \"2022-06-11-cams-atmospheric.nc\", engine=\"netcdf4\", decode_timedelta=True\n",
    ")\n",
    "\n",
    "# Select the zero-hour forecast to get the analysis product.\n",
    "surf_vars_ds = surf_vars_ds.isel(forecast_period=0)\n",
    "atmos_vars_ds = atmos_vars_ds.isel(forecast_period=0)\n",
    "\n",
    "# The file has two time points: UTC 00 and UTC 12. We use both to construct the batch\n",
    "# with time 2022-06-11 UTC 12.\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        # `[None]` inserts a batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_vars_ds[\"t2m\"].values[None]),\n",
    "        \"10u\": torch.from_numpy(surf_vars_ds[\"u10\"].values[None]),\n",
    "        \"10v\": torch.from_numpy(surf_vars_ds[\"v10\"].values[None]),\n",
    "        \"msl\": torch.from_numpy(surf_vars_ds[\"msl\"].values[None]),\n",
    "        \"pm1\": torch.from_numpy(surf_vars_ds[\"pm1\"].values[None]),\n",
    "        \"pm2p5\": torch.from_numpy(surf_vars_ds[\"pm2p5\"].values[None]),\n",
    "        \"pm10\": torch.from_numpy(surf_vars_ds[\"pm10\"].values[None]),\n",
    "        \"tcco\": torch.from_numpy(surf_vars_ds[\"tcco\"].values[None]),\n",
    "        \"tc_no\": torch.from_numpy(surf_vars_ds[\"tc_no\"].values[None]),\n",
    "        \"tcno2\": torch.from_numpy(surf_vars_ds[\"tcno2\"].values[None]),\n",
    "        \"gtco3\": torch.from_numpy(surf_vars_ds[\"gtco3\"].values[None]),\n",
    "        \"tcso2\": torch.from_numpy(surf_vars_ds[\"tcso2\"].values[None]),\n",
    "    },\n",
    "    static_vars={k: torch.from_numpy(v) for k, v in static_vars.items()},\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_vars_ds[\"t\"].values[None]),\n",
    "        \"u\": torch.from_numpy(atmos_vars_ds[\"u\"].values[None]),\n",
    "        \"v\": torch.from_numpy(atmos_vars_ds[\"v\"].values[None]),\n",
    "        \"q\": torch.from_numpy(atmos_vars_ds[\"q\"].values[None]),\n",
    "        \"z\": torch.from_numpy(atmos_vars_ds[\"z\"].values[None]),\n",
    "        \"co\": torch.from_numpy(atmos_vars_ds[\"co\"].values[None]),\n",
    "        \"no\": torch.from_numpy(atmos_vars_ds[\"no\"].values[None]),\n",
    "        \"no2\": torch.from_numpy(atmos_vars_ds[\"no2\"].values[None]),\n",
    "        \"go3\": torch.from_numpy(atmos_vars_ds[\"go3\"].values[None]),\n",
    "        \"so2\": torch.from_numpy(atmos_vars_ds[\"so2\"].values[None]),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(atmos_vars_ds.latitude.values),\n",
    "        lon=torch.from_numpy(atmos_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s.\n",
    "        time=(atmos_vars_ds.valid_time.values.astype(\"datetime64[s]\").tolist()[-1],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f12fe-1e49-49f0-bf03-bffcea9d1551",
   "metadata": {},
   "source": [
    "## Loading and Running the Model\n",
    "\n",
    "Finally, we are ready to load and run the model and visualise the predictions. We perform a roll-out for four steps, which produces predictions for 12 June 2022 UTC 00 and UTC 12 and 13 June 2022 UTC 00 and UTC 12.\n",
    "\n",
    "The model can be run locally, or run on Azure AI Foundry. To run on Foundry, the environment variables `FOUNDRY_ENDPOINT`, `FOUNDRY_TOKEN`, and `BLOB_URL_WITH_SAS` need to be set. If you're unsure on how to set environment variables, see [here](./foundry/intro.md), or simply copy paste your keys below (but be sure not to commit any keys)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dfa402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this according to preference\n",
    "run_on_foundry = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824be34-060d-422a-addf-841c2c3609b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_on_foundry:\n",
    "    from aurora import AuroraAirPollution, rollout\n",
    "\n",
    "    model = AuroraAirPollution()\n",
    "    model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.4-air-pollution.ckpt\")\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        predictions = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=4)]\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "else:\n",
    "    import logging\n",
    "    import os\n",
    "    import warnings\n",
    "\n",
    "    from aurora.foundry import BlobStorageChannel, FoundryClient, submit\n",
    "\n",
    "    # In this demo, we silence all warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # But we do want to show what's happening under the hood!\n",
    "    logging.basicConfig(level=logging.WARNING, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "    logging.getLogger(\"aurora\").setLevel(logging.INFO)\n",
    "\n",
    "    foundry_client = FoundryClient(\n",
    "        endpoint=os.environ[\"FOUNDRY_ENDPOINT\"],\n",
    "        token=os.environ[\"FOUNDRY_TOKEN\"],\n",
    "    )\n",
    "    channel = BlobStorageChannel(os.environ[\"BLOB_URL_WITH_SAS\"])\n",
    "\n",
    "    predictions = list(\n",
    "        submit(\n",
    "            batch,\n",
    "            model_name=\"aurora-0.4-air-pollution\",\n",
    "            num_steps=4,\n",
    "            foundry_client=foundry_client,\n",
    "            channel=channel,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956a27e",
   "metadata": {},
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88fae0-dd72-4a81-bee8-9abe2a42cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    pred = predictions[i]\n",
    "    ax.imshow(pred.surf_vars[\"tcno2\"][0, 0].numpy() / 1e-6, vmin=0, vmax=10, cmap=\"Blues\")\n",
    "    ax.set_title(f\"TC NO${{}}_2$ {pred.metadata.time[0]}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    pred = predictions[i]\n",
    "    ax.imshow(pred.surf_vars[\"pm10\"][0, 0].numpy() / 1e-9, vmin=0, vmax=400, cmap=\"Blues\")\n",
    "    ax.set_title(f\"PM${{}}_{{10}}$ {pred.metadata.time[0]}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab7ef0-8451-4d23-a79d-dee1cc0989ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aurora-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
