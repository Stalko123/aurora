{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a6cb42",
   "metadata": {},
   "source": [
    "# Predictions for Ocean Waves\n",
    "\n",
    "In this example, we run Aurora 0.25Â° Wave on HRES-WAM data for the 16th of September 2022, that is data from ECMWF's ocean wave model called HRES-WAM. We supplement the wave variables with meteorological variables taken from HRES T0 on WeatherBench2. \n",
    "\n",
    "Running this notebook requires additional Python packages. You can install these as follows:\n",
    "\n",
    "```\n",
    "pip install ecmwf-api-client gcsfs zarr cfgrib matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432688f5",
   "metadata": {},
   "source": [
    "## Running this model\n",
    "\n",
    "The model can be run locally using the installed package `microsoft-aurora`, or using an [Azure AI Foundry](https://ai.azure.com/explore/models/Aurora/version/3/registry/azureml?tid=72f988bf-86f1-41af-91ab-2d7cd011db47) version of the model, where a model endpoint is available. Detailed documentation is given at [Aurora on Azure AI Foundry](https://microsoft.github.io/aurora/foundry/intro.html), and a detailed example of Azure AI Foundry usage for the weather prediction model is provided in [this notebook](./foundry/demo_hres_t0.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78830894",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Downloading HRES-WAM From MARS\n",
    "\n",
    "To begin with, register an account with [ECMWF](https://api.ecmwf.int/v1/key) and create `$HOME/.ecmwfapirc` with the following content:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"url\"   : \"https://api.ecmwf.int/v1\",\n",
    "    \"key\"   : <API key>,\n",
    "    \"email\" : <email>\n",
    "}\n",
    "```\n",
    "\n",
    "You can find your API key on your account page.\n",
    "\n",
    "We now download the HRES-WAM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6db9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import ecmwfapi\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads\")\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Day to download. This will download all times for that day.\n",
    "day = \"2022-09-16\"\n",
    "\n",
    "# Set up the variables we want to download\n",
    "variables: dict[str, str] = {\n",
    "    \"swh\": \"140229\",  # Significant wave height\n",
    "    \"pp1d\": \"140231\",  # Peak wave period\n",
    "    \"mwp\": \"140232\",  # Mean wave period\n",
    "    \"mwd\": \"140230\",  # Mean wave direction\n",
    "    \"shww\": \"140234\",  # Significant height of wind waves\n",
    "    \"mdww\": \"140235\",  # Mean direction of wind waves\n",
    "    \"mpww\": \"140236\",  # Mean period of wind waves\n",
    "    \"shts\": \"140237\",  # Significant height of total swell\n",
    "    \"mdts\": \"140238\",  # Mean direction of total swell\n",
    "    \"mpts\": \"140239\",  # Mean period of total swell\n",
    "    \"swh1\": \"140121\",  # Significant wave height of first swell\n",
    "    \"mwd1\": \"140122\",  # Mean direction of first swell\n",
    "    \"mwp1\": \"140123\",  # Mean period of first swell\n",
    "    \"swh2\": \"140124\",  # Significant wave height of second swell\n",
    "    \"mwd2\": \"140125\",  # Mean wave direction of second swell\n",
    "    \"mwp2\": \"140126\",  # Mean wave period of second swell\n",
    "    \"dwi\": \"140249\",  # 10 m neutral wind direction\n",
    "    \"wind\": \"140245\",  # 10 m neutral wind speed\n",
    "}\n",
    "# Convert to form required by MARS.\n",
    "for k, v in variables.items():\n",
    "    assert len(v) == 6\n",
    "    variables[k] = v[3:6] + \".\" + v[0:3]\n",
    "\n",
    "if not (download_path / f\"{day}-wave.grib\").exists():\n",
    "    c = ecmwfapi.ECMWFService(\"mars\")\n",
    "    c.execute(\n",
    "        f\"\"\"\n",
    "        request,\n",
    "            class=od,\n",
    "            date={day}/to/{day},\n",
    "            domain=g,\n",
    "            expver=1,\n",
    "            param={\"/\".join(variables.values())},\n",
    "            stream=wave,\n",
    "            time=00:00:00/06:00:00/12:00:00/18:00:00,\n",
    "            grid=0.25/0.25,\n",
    "            type=an,\n",
    "            target=\"{day}-wave.grib\"\n",
    "        \"\"\",\n",
    "        str(download_path / f\"{day}-wave.grib\"),\n",
    "    )\n",
    "print(\"HRES-WAM data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c568d",
   "metadata": {},
   "source": [
    "## Downloading HRES T0 from WeatherBench2\n",
    "\n",
    "We download the corresponding meteorological variables from HRES T0 on [WeatherBench2](https://weatherbench2.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "\n",
    "# We will download from Google Cloud.\n",
    "url = \"gs://weatherbench2/datasets/hres_t0/2016-2022-6h-1440x721.zarr\"\n",
    "ds = xr.open_zarr(fsspec.get_mapper(url), chunks=None)\n",
    "\n",
    "# Download the surface-level variables. We write the downloaded data to another file to cache.\n",
    "if not (download_path / f\"{day}-surface-level.nc\").exists():\n",
    "    surface_vars = [\n",
    "        \"10m_u_component_of_wind\",\n",
    "        \"10m_v_component_of_wind\",\n",
    "        \"2m_temperature\",\n",
    "        \"mean_sea_level_pressure\",\n",
    "    ]\n",
    "    ds_surf = ds[surface_vars].sel(time=day).compute()\n",
    "    ds_surf.to_netcdf(str(download_path / f\"{day}-surface-level.nc\"))\n",
    "print(\"Surface-level variables downloaded!\")\n",
    "\n",
    "# Download the atmospheric variables. We write the downloaded data to another file to cache.\n",
    "if not (download_path / f\"{day}-atmospheric.nc\").exists():\n",
    "    atmos_vars = [\n",
    "        \"temperature\",\n",
    "        \"u_component_of_wind\",\n",
    "        \"v_component_of_wind\",\n",
    "        \"specific_humidity\",\n",
    "        \"geopotential\",\n",
    "    ]\n",
    "    ds_atmos = ds[atmos_vars].sel(time=day).compute()\n",
    "    ds_atmos.to_netcdf(str(download_path / f\"{day}-atmospheric.nc\"))\n",
    "print(\"Atmospheric variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bf3f3",
   "metadata": {},
   "source": [
    "## Downloading the Static Variables\n",
    "\n",
    "For this model, we also need to include the correct static variables. We have made these available on [HuggingFace](https://huggingface.co/microsoft/aurora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the static variables from HuggingFace.\n",
    "static_path = hf_hub_download(\n",
    "    repo_id=\"microsoft/aurora\",\n",
    "    filename=\"aurora-0.25-wave-static.pickle\",\n",
    ")\n",
    "print(\"Static variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ec640",
   "metadata": {},
   "source": [
    "# Running the Model\n",
    "\n",
    "## Preparing a Batch\n",
    "\n",
    "We convert the downloaded data to an `aurora.Batch`, which is what the model requires. Note again that data is a combination of HRES-WAM ocean wave variables and HRES T0 meteorological variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata\n",
    "\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars = pickle.load(f)\n",
    "surf_vars_ds = xr.open_dataset(\n",
    "    download_path / f\"{day}-surface-level.nc\",\n",
    "    engine=\"netcdf4\",\n",
    "    decode_timedelta=True,\n",
    ")\n",
    "wave_vars_ds = xr.open_dataset(\n",
    "    download_path / f\"{day}-wave.grib\",\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\"indexpath\": \"\"},\n",
    ")\n",
    "atmos_vars_ds = xr.open_dataset(\n",
    "    download_path / f\"{day}-atmospheric.nc\",\n",
    "    engine=\"netcdf4\",\n",
    "    decode_timedelta=True,\n",
    ")\n",
    "\n",
    "\n",
    "def _prepare_hres(x: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Prepare an HRES variable from WeatherBench2.\n",
    "\n",
    "    This does the following things:\n",
    "    * Select the first two time steps: 00:00 and 06:00.\n",
    "    * Insert an empty batch dimension with `[None]`.\n",
    "    * Flip along the latitude axis to ensure that the latitudes are decreasing.\n",
    "    * Copy the data, because the data must be contiguous when converting to PyTorch.\n",
    "    * Convert to PyTorch.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(x[:2][None][..., ::-1, :].copy())\n",
    "\n",
    "\n",
    "def _prepare_wave(x: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Prepare a wave variable.\n",
    "\n",
    "    This does the following things:\n",
    "    * Select the first two time steps: 00:00 and 06:00.\n",
    "    * Insert an empty batch dimension with `[None]`.\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(x[:2][None])\n",
    "\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        \"2t\": _prepare_hres(surf_vars_ds[\"2m_temperature\"].values),\n",
    "        \"10u\": _prepare_hres(surf_vars_ds[\"10m_u_component_of_wind\"].values),\n",
    "        \"10v\": _prepare_hres(surf_vars_ds[\"10m_v_component_of_wind\"].values),\n",
    "        \"msl\": _prepare_hres(surf_vars_ds[\"mean_sea_level_pressure\"].values),\n",
    "        \"swh\": _prepare_wave(wave_vars_ds[\"swh\"].values),\n",
    "        \"mwd\": _prepare_wave(wave_vars_ds[\"mwd\"].values),\n",
    "        \"mwp\": _prepare_wave(wave_vars_ds[\"mwp\"].values),\n",
    "        \"pp1d\": _prepare_wave(wave_vars_ds[\"pp1d\"].values),\n",
    "        \"shww\": _prepare_wave(wave_vars_ds[\"shww\"].values),\n",
    "        \"mdww\": _prepare_wave(wave_vars_ds[\"mdww\"].values),\n",
    "        \"mpww\": _prepare_wave(wave_vars_ds[\"mpww\"].values),\n",
    "        \"shts\": _prepare_wave(wave_vars_ds[\"shts\"].values),\n",
    "        \"mdts\": _prepare_wave(wave_vars_ds[\"mdts\"].values),\n",
    "        \"mpts\": _prepare_wave(wave_vars_ds[\"mpts\"].values),\n",
    "        \"swh1\": _prepare_wave(wave_vars_ds[\"swh1\"].values),\n",
    "        \"mwd1\": _prepare_wave(wave_vars_ds[\"mwd1\"].values),\n",
    "        \"mwp1\": _prepare_wave(wave_vars_ds[\"mwp1\"].values),\n",
    "        \"swh2\": _prepare_wave(wave_vars_ds[\"swh2\"].values),\n",
    "        \"mwd2\": _prepare_wave(wave_vars_ds[\"mwd2\"].values),\n",
    "        \"mwp2\": _prepare_wave(wave_vars_ds[\"mwp2\"].values),\n",
    "        \"wind\": _prepare_wave(wave_vars_ds[\"wind\"].values),\n",
    "        \"dwi\": _prepare_wave(wave_vars_ds[\"dwi\"].values),\n",
    "    },\n",
    "    static_vars={k: torch.from_numpy(v) for k, v in static_vars.items()},\n",
    "    atmos_vars={\n",
    "        \"t\": _prepare_hres(atmos_vars_ds[\"temperature\"].values),\n",
    "        \"u\": _prepare_hres(atmos_vars_ds[\"u_component_of_wind\"].values),\n",
    "        \"v\": _prepare_hres(atmos_vars_ds[\"v_component_of_wind\"].values),\n",
    "        \"q\": _prepare_hres(atmos_vars_ds[\"specific_humidity\"].values),\n",
    "        \"z\": _prepare_hres(atmos_vars_ds[\"geopotential\"].values),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        # Flip the latitudes from WeatherBench2! We need to copy because converting\n",
    "        # to PyTorch, because the data must be contiguous.\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values[::-1].copy()),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element. Select element 1, corresponding to time\n",
    "        # 06:00.\n",
    "        time=(surf_vars_ds.time.values.astype(\"datetime64[s]\").tolist()[1],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400ba1",
   "metadata": {},
   "source": [
    "## Loading and Running the Model\n",
    "\n",
    "Finally, we are ready to load and run the model and visualise the predictions. We perform a roll-out for two steps, which produces predictions for hours 12:00 and 18:00.\n",
    "\n",
    "Note that some specific postprocessing is required for the HRES-WAM ocean wave variables. See Section C.5 of the [Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09005-y/MediaObjects/41586_2025_9005_MOESM1_ESM.pdf).\n",
    "\n",
    "The model can be run locally, or run on Azure AI Foundry. To run on Foundry, the environment variables `FOUNDRY_ENDPOINT`, `FOUNDRY_TOKEN`, and `BLOB_URL_WITH_SAS` need to be set. If you're unsure on how to set environment variables, see [here](./foundry/intro.md), or simply copy paste your keys below (but be sure not to commit any keys)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this according to preference\n",
    "run_on_foundry = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bec0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_on_foundry:\n",
    "    from aurora import AuroraWave, rollout\n",
    "\n",
    "    model = AuroraWave()\n",
    "    model.load_checkpoint()\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        predictions = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=2)]\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "else:\n",
    "    import logging\n",
    "    import os\n",
    "    import warnings\n",
    "\n",
    "    from aurora.foundry import BlobStorageChannel, FoundryClient, submit\n",
    "\n",
    "    # In this demo, we silence all warnings.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # But we do want to show what's happening under the hood!\n",
    "    logging.basicConfig(level=logging.WARNING, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "    logging.getLogger(\"aurora\").setLevel(logging.INFO)\n",
    "\n",
    "    foundry_client = FoundryClient(\n",
    "        endpoint=os.environ[\"FOUNDRY_ENDPOINT\"],\n",
    "        token=os.environ[\"FOUNDRY_TOKEN\"],\n",
    "    )\n",
    "    channel = BlobStorageChannel(os.environ[\"BLOB_URL_WITH_SAS\"])\n",
    "\n",
    "    predictions = list(\n",
    "        submit(\n",
    "            batch,\n",
    "            model_name=\"aurora-0.25-wave\",\n",
    "            num_steps=2,\n",
    "            foundry_client=foundry_client,\n",
    "            channel=channel,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2226787",
   "metadata": {},
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 6.5))\n",
    "\n",
    "for i in range(axs.shape[0]):\n",
    "    pred = predictions[i]\n",
    "\n",
    "    ax = axs[i, 0]\n",
    "    ax.imshow(pred.surf_vars[\"mwd\"][0, 0].numpy(), vmin=0, vmax=360, cmap=\"twilight\")\n",
    "    ax.set_ylabel(str(pred.metadata.time[0]))\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Aurora Prediction\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax = axs[i, 1]\n",
    "    ref = wave_vars_ds[\"mwd\"][2 + i].values\n",
    "    # If the wave has practically zero magnitude, we say that the wave direction is missing.\n",
    "    # See Section C.5 of the Supplementary Information.\n",
    "    ref[wave_vars_ds[\"swh\"][2 + i].values < 1e-4] = np.nan\n",
    "    ax.imshow(ref, vmin=0, vmax=360, cmap=\"twilight\")\n",
    "    if i == 0:\n",
    "        ax.set_title(\"HRES-WAM\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac80f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aurora-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
